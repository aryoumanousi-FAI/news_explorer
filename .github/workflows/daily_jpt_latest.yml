# .github/workflows/daily_scrape.yml
name: Daily JPT + WorldOil scrape and merge

on:
  schedule:
    # 14:00 UTC daily
    - cron: "0 14 * * *"
  workflow_dispatch:

permissions:
  contents: write

jobs:
  scrape-and-merge:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Scrape daily data (JPT + WorldOil) into jpt_daily.csv
        env:
          # run both spiders daily
          SPIDERS: "jpt_latest,worldoil_news"
          # keep this light; hard-stop prevents paging deep once history exists
          MAX_PAGES: "10"
          STOP_AT_LAST_DATE: "1"
        run: |
          python scripts/scrape_new.py

      - name: Merge master + daily into jpt.csv
        run: |
          python scripts/merge_three_way.py

      - name: Commit and push changes
        run: |
          git config --global user.name "github-actions[bot]"
          git config --global user.email "github-actions[bot]@users.noreply.github.com"

          git add jpt_scraper/data/jpt_daily.csv
          git add jpt_scraper/data/jpt.csv

          if git diff --staged --quiet; then
            echo "No changes to commit."
          else
            git commit -m "Auto-update: daily scrape + merged [skip ci]"
            git push
          fi
